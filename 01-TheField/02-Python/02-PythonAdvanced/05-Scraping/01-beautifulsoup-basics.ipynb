{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic data collection on the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start to tackle some nice web pages (HTML), we will discover the XML language which is a good introduction to scraping data on the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lists a few properties of the XML language.\n",
    "\n",
    "- XML was created to facilitate data exchange between machines and software.\n",
    "\n",
    "- XML is a language that is written using tags.\n",
    "\n",
    "- XML is a W3C recommendation, so it is a technology with strict rules to follow.\n",
    "\n",
    "- XML is intended to be understandable by everyone: people and machines alike.\n",
    "\n",
    "- XML allows us to create our own vocabulary using a set of customizable rules and tags.\n",
    "\n",
    "- XML is also compatible with the web so that data exchanges can be easily carried out over the Internet.\n",
    "\n",
    "- XML is therefore standardized, simple, but above all extensible and configurable so that any type of data can be described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a XML document, which we have saved as `data.xml` in the `assets/` directory.\n",
    "\n",
    "Display its content!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<users>\n",
      "    <user data-id=\"101\">\n",
      "        <name>Zorro</name>\n",
      "        <job>Dancer</job>\n",
      "    </user>\n",
      "    <user data-id=\"102\">\n",
      "        <name>Hulk</name>\n",
      "        <job>Football player</job>\n",
      "    </user>\n",
      "    <user data-id=\"103\">\n",
      "        <name>Zidane</name>\n",
      "        <job>Star</job>\n",
      "    </user>\n",
      "    <user data-id=\"104\">\n",
      "        <name>Beans</name>\n",
      "        <job>Grocer</job>\n",
      "    </user>\n",
      "    <user data-id=\"105\">\n",
      "        <name>Batman</name>\n",
      "        <job>Veterinary</job>\n",
      "    </user>\n",
      "    <user data-id=\"106\">\n",
      "        <name>Spiderman</name>\n",
      "        <job>Veterinary</job>\n",
      "    </user>\n",
      "</users>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"./assets/data.xml\"\n",
    "file = open(filename, \"r\")\n",
    "print(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line indicates the encoding (we always stay in the UTF-8 encoding). Then we notice that the \"users\" tag has other \"user\" tags that themselves have their own tags. The data is hierarchized in a tree and each node provides information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small script that displays all the usernames.\n",
    "\n",
    "You will first have to install the `lxml` package. It depends on the `numpy` package, which will be installed alongside `lxml` if you use a standard package manager. However, some version of `numpy` give problems, so changing the version might be the first thing that you can troubleshoot if you fail to import `lxml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zorro\n",
      "Hulk\n",
      "Zidane\n",
      "Beans\n",
      "Batman\n",
      "Spiderman\n"
     ]
    }
   ],
   "source": [
    "from lxml import etree\n",
    "\n",
    "# I define my source document\n",
    "tree = etree.parse(filename)\n",
    "# I look at my document and identify the path to the tag to get to the \"user\" information\n",
    "\n",
    "# The user name we are looking for is in it's own tag, `name`. Which itself\n",
    "# is in a tag `user`, and lastly `user` is contained in a `users` tag.\n",
    "# So tree.xpath(\"/users/user/name\") contains the tags associated with our search\n",
    "for user in tree.xpath(\"/users/user/name\"):\n",
    "    # I only want to display the content (.text) of the `/users/user/name` tags\n",
    "    print(user.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.xpath(\"/users/user/name\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can display the attributes of the tags that store this information\n",
    "tree = etree.parse(filename)\n",
    "for user in tree.xpath(\"/users/user\"):\n",
    "    print(user.get(\"data-id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can refine the display by proposing to display only users whose job is Veterinary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = etree.parse(filename)\n",
    "# Quel joli petit dictionnaire\n",
    "for user in tree.xpath(\"/users/user[job='Veterinary']/name\"):\n",
    "    print(user.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping\n",
    "\n",
    "Web scraping is a technique for automatically extracting information from websites. It is very useful in a lot of use cases:\n",
    "\n",
    "- **E-commerce**: create Excel sheet gathering all products from a provider's website\n",
    "- **Business prospecting**: extract contact information from websites (phone numbers, e-mail addresses...)\n",
    "- **Media analysis**: collect articles from online newspapers on a daily basis\n",
    "- ...\n",
    "\n",
    "Of course this task could be done by a human but this would be very painful and repetitive especially when the number of targeted pages is huge. To save time, we can automate it and, as usual, Python has a solution for that.\n",
    "\n",
    "The idea is to automate the human task by **fetching** pages and **extracting** data from them.\n",
    "\n",
    "**Fetching**\n",
    "\n",
    "Fetching is the downloading of the `html` source code of a page. It's exactly what your browser does when you open a website. To test it, open your browser, right-click anywhere on the page and select `view page source`. This is what Python will automate !\n",
    "\n",
    "**Extraction**\n",
    "\n",
    "From the `html` page you can extract, transform and load information in Python exactly like we did before with `xml`.\n",
    "\n",
    "Let's implement those two steps in few lines of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching: Scraping via HTTP requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTTP (HyperText Transfer Protocol) is a protocol that will allow a **client** (you, through your browser or your code for example) to communicate with a **server** connected to the network (hosting a website or an internet document)\n",
    "\n",
    "Requests always go in pairs: the request (from the client) and the response (from the server).\n",
    "If this is not the case, it is because a problem has occurred at a point in the network.\n",
    "\n",
    "In Python we can use the library `requests` with the method `get`. It will emulate what you are doing manually in your browser.\n",
    "\n",
    "Start by installing the library using `pip install requests` or the `conda` package manager.\n",
    "\n",
    "Then let's take an URL, download the content and display it to see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the content is not easily readable. But like `xml`, `html` is a structured representation of the content. We then can use Python for reading it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction using Beautifulsoup\n",
    "\n",
    "For parsing `xml`we have used `xpath`. This is also possible for `html` ([see here](https://python-docs.readthedocs.io/en/latest/scenarios/scrape.html)).\n",
    "\n",
    "However there is a more _user-friendly_ library in Python that does the job quite well: `beautifulsoup`. We will use it in this notebook.\n",
    "\n",
    "First install it using `pip install beautifulsoup4` or the `conda` package manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the website content we just scraped in a Beautifulsoup object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m----> 2\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(content, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'content' is not defined"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(content, \"html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `html` tree of the page is now loaded and we can access its information. For example we can get the title of the page by recovering the content of the tag `h1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# We only retrieve the content ==> .text\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tag\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "for tag in soup.find_all(\"h1\"):\n",
    "    # We only retrieve the content ==> .text\n",
    "    print(tag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same with `h2` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# We only retrieve the content ==> .text\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tag\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "for tag in soup.find_all(\"h2\"):\n",
    "    # We only retrieve the content ==> .text\n",
    "    print(tag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, do the same with the `p` tags (that stand for _paragraphs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Choose an URL, get its content by using `requests`. Load it by using `beautifulsoup`. Then create a summary of the page by printing the content of `h1`, the first `h2` and the first paragraph (`p`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done, you did it ! Let's go with more advanced operation using `beautifulsoup`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "661c13da0699b4d3adfbe1192573631e3fbd9fa55405ad8c238e615a4e7e8a33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
